[progress]   5% | snapshot | finding latest HF snapshot
[progress]  15% | config | loading model and tokenizer config
[progress]  35% | weights | indexing safetensors headers
[vspec-chat] snapshot= C:\Users\Long\.cache\huggingface\hub\models--Qwen--Qwen3-8B\snapshots\b968826d9c46dd6066d109eabc6255188de91218
[vspec-chat] adapter= generic
[vspec-chat] model_type= qwen3
[vspec-chat] tensors= 399
[vspec-chat] vocab_size= 151669
[vspec-chat] dtype_stats= {'bf16': 399}
[vspec-chat] lang_mode= vi
[vspec-chat] chat_format= auto
[vspec-chat] speed_preset= ultra
[vspec-chat] decode_top_k= 16
[vspec-chat] decode_lang_top_n= 64
[vspec-chat] prompt_tokens= 40
[progress]  36% | runtime-load | layer 0/1
[progress]  58% | runtime-load | layer 1/1
[vspec-chat] runtime_device= cuda
[progress]  60% | runtime | vspec-torch-cuda
[progress]  80% | prefill | 39/39 tokens
[progress]  82% | decode | max_steps=1
[progress]  99% | decode | 1/1 tokens
[progress] 100% | done | generation complete
[vspec-chat] output:
nehmer
[vspec-chat] runtime_mode= vspec-torch-cuda
[vspec-chat] runtime_is_vspec= True
[vspec-chat] timing_sec= 1.8443
[vspec-chat] tokens_prompt= 40
[vspec-chat] tokens_generated= 1
[vspec-chat] tokens_total= 41
[vspec-chat] tokens_per_sec= 0.5422
[vspec-chat] processing_leq_4bit= False
[vspec-chat] note=weights are not <=4-bit in this path (mostly fp16/bf16/fp32)
